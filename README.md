# Replication of Transformer Uncertainty
This project replicates the results in the paper "[Understanding Neural Abstractive Summarization Models via Uncertainty](https://arxiv.org/abs/2010.07882)"
by Jiacheng Xu, Shrey Desai, and Greg Durrett.

There original code can be found at: [https://github.com/jiacheng-xu/text-sum-uncertainty](https://github.com/jiacheng-xu/text-sum-uncertainty)

## Setup
Setup (python 3.9.1). Clone the repository and install requirements.
```
git clone
pip install -r requirements.txt
```

## Datasets
Datasets are loaded from HuggingFace with the following commands:
```
from datasets import load_dataset

load_dataset("xsum")
load_dataset("cnn_dailymail")
```

## Models
Experiments use the two models PEGASUS and BART. I use HuggingFace for building these two models.
1. [PEGASUS CNN Dailymail](https://huggingface.co/google/pegasus-cnn_dailymail)
2. [PEGASUS XSum](https://huggingface.co/google/pegasus-xsum)
3. [BART Large CNN](https://huggingface.co/facebook/bart-large-cnn)
4. [BART Large XSum](https://huggingface.co/facebook/bart-large-xsum)

## Entropy
Entropy is calculated using the Shannon entropy for a probability distribution. Nucleus sampling
is used to sample only the top 1 - p most probable outcomes. Nucleus sampling is set through
a HuggingFace parameter during model generation. For example:
```
model_output = model.generate(
        input_token_ids,
        num_beams=num_beams,
        return_dict_in_generate=True,
        output_scores=True,
        top_p=0.95,
    )
```
top_p is nucleus sampling in HuggingFace.

## Analysis
### Model Uncertainty during Generation
In this part, I attempt replication of the behavior of BART and PEGASUS in how they perform a mixture
of copying and generating tokens.

In the paper, the authors state that they compute their figures on 10k generation steps for each of the
models. In selecting the source documents for token generation, I randomly select without replacement
a document from the test set of either CNN/Daily Mail or XSum datasets. Once the token generation has
reached 10k steps, I stop the calculation of entropy and plot the figures.

For the first figure, I separate the output bigrams into two buckets: (1) if they have been generated
from existing bigrams in the source document or (2) if they are novel bigrams generated by the model.

For the second figure, I create 10 buckets from 0.0 to 0.9 to indicate what part of the sentence the
token is located. Both figure 1 and figure 2 are calculated from the same source documents.

#### Figure 1 from Original Paper
![Original Bigram Prediction Engropy](https://raw.githubusercontent.com/vincehartman38/Replication-of-Transformer-Uncertainty/main/original_figures/replication_figure1.jpg)

#### Replicated Figures

These results align with the original paper in that existing bigrams have lower entropy actions
than novel bigrams. In the figures, this is seen by the median entropy for Existing Bigrams being consistently
closer to 0 when compared to the median of Novel Bigrams. The XSum dataset for both PEGASUS and BART
have more more novel bigrams than the CNN / DailyMail dataset. For XSum, the distributions tend to be closer
to a normal distribuition than the CNN / DailyMail distributions.

![Replication Figure 1 BART CNN](https://raw.githubusercontent.com/vincehartman38/Replication-of-Transformer-Uncertainty/main/results/bart-large-cnn_histogram_20220319-170858.jpeg)

![Replication Figure 1 PEGASUS CNN](https://raw.githubusercontent.com/vincehartman38/Replication-of-Transformer-Uncertainty/main/results/pegasus-cnn_dailymail_histogram_20220319-174830.jpeg)
