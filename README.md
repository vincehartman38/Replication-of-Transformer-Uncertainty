# Replication of Transformer Uncertainty
This project replicates the results in the paper "[Understanding Neural Abstractive Summarization Models via Uncertainty](https://arxiv.org/abs/2010.07882)"
by Jiacheng Xu, Shrey Desai, and Greg Durrett.

There original code can be found at: [https://github.com/jiacheng-xu/text-sum-uncertainty](https://github.com/jiacheng-xu/text-sum-uncertainty)

## Setup
Setup (python 3.9.1). Clone the repository and install requirements.
```
git clone
pip install -r requirements.txt
python -m spacy download en_core_web_trf
```

## Datasets
Datasets are loaded from HuggingFace with the following commands:
```
from datasets import load_dataset

load_dataset("xsum")
load_dataset("cnn_dailymail")
```

## Models
Experiments use the two models PEGASUS and BART. I use HuggingFace for building these two models.
1. [PEGASUS CNN Dailymail](https://huggingface.co/google/pegasus-cnn_dailymail)
2. [PEGASUS XSum](https://huggingface.co/google/pegasus-xsum)
3. [BART Large CNN](https://huggingface.co/facebook/bart-large-cnn)
4. [BART Large XSum](https://huggingface.co/facebook/bart-large-xsum)

## Entropy
Entropy is calculated using the Shannon entropy for a probability distribution. Nucleus sampling
is used to sample only the top 1 - p most probable outcomes. Nucleus sampling is set through
a HuggingFace parameter during model generation. For example:
```
model_output = model.generate(
        input_token_ids,
        num_beams=num_beams,
        return_dict_in_generate=True,
        output_scores=True,
        top_p=0.95,
    )
```
top_p is nucleus sampling in HuggingFace.

## Analysis
### Model Uncertainty during Generation
In this part, I attempt replication of the behavior of BART and PEGASUS in how they perform a mixture
of copying and generating tokens.

In the paper, the authors state that they compute their figures on 10k generation steps for each of the
models. In selecting the source documents for token generation, I randomly select without replacement
a document from the test set of either CNN/Daily Mail or XSum datasets. Once the token generation has
reached 10k steps, I stop the calculation of entropy and plot the figures.

For the first figure, I separate the output bigrams into two buckets: (1) if they have been generated
from existing bigrams in the source document or (2) if they are novel bigrams generated by the model.

For the second figure, I create 10 buckets from 0.0 to 0.9 to indicate what part of the sentence the
token is located. Both figure 1 and figure 2 are calculated from the same source documents.

#### Figure 1 from Original Paper
![Original Bigram Prediction Engropy](https://raw.githubusercontent.com/vincehartman38/Replication-of-Transformer-Uncertainty/main/original_figures/replication_figure1.jpg)

#### Replicated Figures for Bigram Entropy Extracted/Novel

These results align with the original paper in that existing bigrams have lower entropy actions
than novel bigrams. In the figures, this is seen by the median entropy for Existing Bigrams being consistently
closer to 0 when compared to the median of Novel Bigrams. The XSum dataset for both PEGASUS and BART
have more more novel bigrams than the CNN / DailyMail dataset. For XSum, the distributions tend to be closer
to a normal distribuition than the CNN / DailyMail distributions.

Images are listed in order of left to right compared to original one.  

DATASET | PEGASUS | BART 
:-------------------------:|:-------------------------:| :-------------------------:
CNN/DM | <img src="https://raw.githubusercontent.com/vincehartman38/Replication-of-Transformer-Uncertainty/main/results/pegasus-cnn_dailymail_histogram_20220319-174830.jpeg" width=500 alt="Replication Figure 1 PEGASUS CNN"> | <img src="https://raw.githubusercontent.com/vincehartman38/Replication-of-Transformer-Uncertainty/main/results/bart-large-cnn_histogram_20220319-170858.jpeg" width=500 alt="Replication Figure 1 BART CNN">
XSum | <img src="https://raw.githubusercontent.com/vincehartman38/Replication-of-Transformer-Uncertainty/main/results/pegasus-xsum_hisotgram_20220319-232155.jpeg" width=500 alt="Replication Figure 1 PEGASUS XSUM">  |  <img src="https://raw.githubusercontent.com/vincehartman38/Replication-of-Transformer-Uncertainty/main/results/bart-large-xsum_histogram_20220319-193658.jpeg" width=500 alt="Replication Figure 1 BART XSUM">

#### Figure 2 from Original Paper
![Original Bigram Prediction Entropy](https://raw.githubusercontent.com/vincehartman38/Replication-of-Transformer-Uncertainty/main/original_figures/replication_figure2.jpg)

#### Replicated Figures for Entropy for Sentence Position

DATASET | PEGASUS | BART 
:-------------------------:|:-------------------------:| :-------------------------:
CNN/DM | <img src="https://raw.githubusercontent.com/vincehartman38/Replication-of-Transformer-Uncertainty/main/results/pegasus-cnn_dailymail_boxplot_20220319-174851.jpeg" width=500 alt="Replication Figure 2 PEGASUS CNN"> |<img src="https://raw.githubusercontent.com/vincehartman38/Replication-of-Transformer-Uncertainty/main/results/bart-large-cnn_boxplot_20220319-170924.jpeg" width=500 alt="Replication Figure 2 BART CNN">
XSum | <img src="https://raw.githubusercontent.com/vincehartman38/Replication-of-Transformer-Uncertainty/main/results/pegasus-xsum_boxplot_20220319-232201.jpeg" width=500 alt="Replication Figure 2 PEGASUS XSUM"> |<img src="https://raw.githubusercontent.com/vincehartman38/Replication-of-Transformer-Uncertainty/main/results/bart-large-xsum_boxplot_20220319-193712.jpeg" width=500 alt="Replication Figure 2 BART XSUM">

### Entropies of Syntactic Productions
I used the summaries generated from the first part with the [Berkely Neural Parser](https://github.com/nikitakit/self-attentive-parser)
and explore the connection between syntax and uncertainty.

#### Figure 3 from Original Paper
![Original Syntactic Distance](https://raw.githubusercontent.com/vincehartman38/Replication-of-Transformer-Uncertainty/main/original_figures/replication_figure3.jpg)

#### Replicated Figures for Syntactic Distance
